{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Tutorial\n",
    "\n",
    "- Built to run on multiple CPUs or GPUs and mobile operating systems\n",
    "- Written in c++, but has several wrappers in many languages\n",
    "\n",
    "- A model is represented as a data flow graph\n",
    "- The graph contains a set of nodes called operations, such as addition, multiplication, nonlinear functions\n",
    "- Each operation takes as input a tensor and outputs a tensor\n",
    "- Tensor: A multidimensional array of numbers, how data is represented in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "\n",
    "# Import a helper function from TensorFlow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Download the data, save it to the mnist_data folder,\n",
    "# and process it so that data is in one-hot encoded format.\n",
    "mnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True)\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding: Our y variable for each sample would appear as follows:\n",
    "#   1 2 3 4 5 6 7 8 9 0\n",
    "# [ 1 0 0 0 0 0 0 0 0 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0 # Not sure why Google does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 30\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: A variable that we'll assign data to at a later date.\n",
    "# It's never initialized and contains no data.\n",
    "# Here, we pass in the type and shape of our data as parameters.\n",
    "#\n",
    "# TensorFlow graph input\n",
    "# `None` is the batch size (the number of examples for the current batch)\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "# Weights are probabilities that affect how data flows in the graph and will continuously\n",
    "# be updated so that our results get closer to the solution.\n",
    "# The bias let's us shift our regression line to better fit the data.\n",
    "#\n",
    "# Set model weights and biases\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A named scope helps us organize nodes in the graph visualizer called TensorBoard.\n",
    "# We'll create three scopes:\n",
    "# 1) Implement softmax regression model by matrix multiplying input images X by\n",
    "# weight matrix W and then adding the bias b.\n",
    "#\n",
    "# Create a named scope\n",
    "with tf.name_scope(\"xW_b\") as scope:\n",
    "    # Construct a linear model\n",
    "    model = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    \n",
    "# Summary operations that help us visualize the distribution of our weights and biases.\n",
    "#\n",
    "# Add summary operations to collect data\n",
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Create our cost function to help minimize error during training.\n",
    "#\n",
    "# More name scopes will clean up graph representation\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    # Minimize error using cross entropy\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    # Create a summary to monitor the cost function\n",
    "    tf.summary.scalar(\"cost_function\", cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create our optimization function that will let our model improve during training.\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables (assign their default values)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single operator\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, loss = 29.275465, W_sum = 0.000157, b_sum = 0.000003\n",
      "Epoch: 0002, loss = 21.989802, W_sum = 0.000303, b_sum = 0.000005\n",
      "Epoch: 0003, loss = 20.949330, W_sum = 0.000477, b_sum = 0.000009\n",
      "Epoch: 0004, loss = 20.974767, W_sum = 0.000619, b_sum = 0.000011\n",
      "Epoch: 0005, loss = 19.965329, W_sum = 0.000755, b_sum = 0.000007\n",
      "Epoch: 0006, loss = 20.116556, W_sum = 0.000921, b_sum = 0.000008\n",
      "Epoch: 0007, loss = 19.668701, W_sum = 0.001192, b_sum = 0.000014\n",
      "Epoch: 0008, loss = 19.471331, W_sum = 0.001387, b_sum = 0.000014\n",
      "Epoch: 0009, loss = 19.544566, W_sum = 0.001581, b_sum = 0.000011\n",
      "Epoch: 0010, loss = 19.309294, W_sum = 0.001801, b_sum = 0.000015\n",
      "Epoch: 0011, loss = 18.704587, W_sum = 0.002003, b_sum = 0.000020\n",
      "Epoch: 0012, loss = 19.326377, W_sum = 0.002235, b_sum = 0.000021\n",
      "Epoch: 0013, loss = 18.729991, W_sum = 0.002466, b_sum = 0.000026\n",
      "Epoch: 0014, loss = 18.696948, W_sum = 0.002720, b_sum = 0.000021\n",
      "Epoch: 0015, loss = 19.011172, W_sum = 0.002939, b_sum = 0.000017\n",
      "Epoch: 0016, loss = 18.493727, W_sum = 0.003181, b_sum = 0.000019\n",
      "Epoch: 0017, loss = 18.780251, W_sum = 0.003506, b_sum = 0.000024\n",
      "Epoch: 0018, loss = 18.834713, W_sum = 0.003768, b_sum = 0.000026\n",
      "Epoch: 0019, loss = 18.472886, W_sum = 0.004027, b_sum = 0.000027\n",
      "Epoch: 0020, loss = 18.265021, W_sum = 0.004354, b_sum = 0.000031\n",
      "Epoch: 0021, loss = 18.495762, W_sum = 0.004678, b_sum = 0.000033\n",
      "Epoch: 0022, loss = 18.724176, W_sum = 0.004938, b_sum = 0.000037\n",
      "Epoch: 0023, loss = 18.142996, W_sum = 0.005261, b_sum = 0.000036\n",
      "Epoch: 0024, loss = 18.384565, W_sum = 0.005600, b_sum = 0.000040\n",
      "Epoch: 0025, loss = 18.075026, W_sum = 0.005877, b_sum = 0.000047\n",
      "Epoch: 0026, loss = 18.653697, W_sum = 0.006230, b_sum = 0.000047\n",
      "Epoch: 0027, loss = 18.167407, W_sum = 0.006596, b_sum = 0.000051\n",
      "Epoch: 0028, loss = 17.996802, W_sum = 0.006896, b_sum = 0.000058\n",
      "Epoch: 0029, loss = 18.243280, W_sum = 0.007230, b_sum = 0.000059\n",
      "Epoch: 0030, loss = 17.959135, W_sum = 0.007581, b_sum = 0.000062\n",
      "Accuracy: 0.9115\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initializing a session let's us execute our data flow graph\n",
    "    sess.run(init)\n",
    "\n",
    "    # Change this to a location on your computer.\n",
    "    # We'll later visualize the data from this directory using TensorBoard.\n",
    "    summary_writer = tf.summary.FileWriter('logs', graph=sess.graph)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Compute the total number of batches\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            # Retrieve the current batch of training samples\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit our model using batch data in the gradient descent algorithm\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "            # Compute the average loss for the current batch\n",
    "            cost = sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += cost/total_batch\n",
    "            \n",
    "            # Write logs for each epoch\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, epoch*total_batch + i)\n",
    "            \n",
    "        # Display logs per display step, to ensure our model is improving during training\n",
    "        if epoch % display_step == 0:\n",
    "            print(f'Epoch: {(epoch + 1):04d}, loss = {avg_cost:.6f}, '\n",
    "                  f'W_sum = {sess.run(tf.reduce_sum(W)):.6f}, '\n",
    "                  f'b_sum = {sess.run(tf.reduce_sum(b)):.6f}')\n",
    "\n",
    "    # Test the model by comparing model values to output values\n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.9.0 at http://Marifels-MBP-2.home:6006 (Press CTRL+C to quit)\n",
      "\u001b[33mW0915 13:21:53.930725 Thread-2 application.py:293] path /robots.txt not found, sending 404\n",
      "\u001b[0m\u001b[33mW0915 13:21:56.663722 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:21:56.664203 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:04.970661 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:04.970929 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:12.926516 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:12.928028 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:15.075754 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:15.076113 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:18.451690 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:18.452029 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:25.062224 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:25.062909 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:37.401403 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:37.402112 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:48.727472 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:48.728101 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:58.157724 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0915 13:22:58.163442 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "# Run the following line to visualize the model in TensorBoard\n",
    "!tensorboard --logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events: Shows the output of cost function over time\n",
    "# Histograms: Shows the variance in weights and biases over time\n",
    "# Graphs: Shows the created graph and variables for the weights and biases\n",
    "# (double-click for a detailed view)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
